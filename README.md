# ğŸš€ Projeto de MigraÃ§Ã£o RH â€“ ETL com Apache Airflow

Este projeto implementa um pipeline ETL (Extract, Transform, Load) para migraÃ§Ã£o de dados de Recursos Humanos de um banco PostgreSQL para MySQL, utilizando Apache Airflow e Pandas.

---

## ğŸ“‹ VisÃ£o Geral

O objetivo Ã© automatizar o processo de migraÃ§Ã£o de dados entre bancos, garantindo integridade, consistÃªncia e rastreabilidade.  
O fluxo consiste em extrair, transformar e carregar informaÃ§Ãµes de tabelas do sistema de RH.

Tabelas migradas:
ğŸ¢ departamentos - InformaÃ§Ãµes sobre os departamentos da empresa  
ğŸ’¼ cargos - Dados de cargos e posiÃ§Ãµes  
ğŸ‘¨â€ğŸ’¼ funcionarios - Cadastro completo de funcionÃ¡rios  
ğŸ’° salarios - HistÃ³rico e informaÃ§Ãµes salariais  

---

## ğŸ§© Tecnologias Utilizadas

ğŸŒ€ Apache Airflow - OrquestraÃ§Ã£o do pipeline ETL  
ğŸ³ Docker / Docker Compose - ContainerizaÃ§Ã£o e ambiente de execuÃ§Ã£o  
ğŸ Python - Linguagem principal do projeto  
ğŸ§  Pandas - TransformaÃ§Ã£o e manipulaÃ§Ã£o de dados  
ğŸ˜ PostgreSQL - Banco de dados de origem  
ğŸ¬ MySQL - Banco de dados de destino  

---

## âš™ï¸ ConfiguraÃ§Ã£o e InstalaÃ§Ã£o

### ğŸ”§ PrÃ©-requisitos
- Docker  
- Docker Compose  

---

### ğŸ“¦ Passos para ExecuÃ§Ã£o

1. Clone o repositÃ³rio:
   git clone <url-do-repositorio>  
   cd migracao-rh  

2. Configure os bancos de dados:
   - Configure seu PostgreSQL e MySQL locais.  
   - Certifique-se de que as credenciais estejam corretas para conexÃ£o via UI do Airflow.  

3. Inicie o Airflow com Docker:
   docker-compose up -d  

4. Acesse o Airflow:
   Abra o navegador e vÃ¡ para: http://localhost:8080  

5. Verifique as DAGs:
   - Na interface do Airflow, localize a DAG de migraÃ§Ã£o.  
   - Ative a DAG e clique em "Trigger DAG" para iniciar a execuÃ§Ã£o.  

6. Acompanhe o pipeline:
   - Utilize o grÃ¡fico da DAG no Airflow para visualizar cada etapa (Extract â†’ Transform â†’ Load).  
   - Consulte os logs das tasks para verificar o progresso e possÃ­veis erros.  

7. Valide os dados no MySQL:
   - ApÃ³s a execuÃ§Ã£o, acesse o banco de destino e valide se as tabelas foram carregadas corretamente.  
   - Use um cliente SQL ou o prÃ³prio terminal para conferir:  
     SELECT * FROM funcionarios LIMIT 10;  

---

## ğŸ“ˆ Estrutura do Pipeline

O processo ETL Ã© composto pelas seguintes etapas:

1. Extract â€“ Leitura dos dados do PostgreSQL  
2. Transform â€“ Limpeza e padronizaÃ§Ã£o dos dados com Pandas  
3. Load â€“ InserÃ§Ã£o dos dados tratados no MySQL  

O Apache Airflow coordena essas etapas garantindo execuÃ§Ã£o ordenada, logs e monitoramento visual.

---

## ğŸ§  BenefÃ­cios do Projeto

- AutomaÃ§Ã£o completa da migraÃ§Ã£o de dados  
- Garantia de consistÃªncia e integridade  
- Escalabilidade e fÃ¡cil manutenÃ§Ã£o  
- Monitoramento via UI do Airflow  
- Registro detalhado de execuÃ§Ã£o e falhas  